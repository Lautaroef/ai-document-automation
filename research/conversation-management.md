# OpenAI Conversation Management: Complete Research Guide (2025)

**Date:** November 5, 2025
**Research Focus:** Finding the simplest, most native way to handle multi-turn conversations with OpenAI APIs

---

## Executive Summary

**The Answer:** OpenAI has introduced THREE native approaches for conversation management in 2025:

1. **Conversations API** - Server-side persistent conversation objects (BETA, most native)
2. **`previous_response_id` chaining** - Simple response linking without explicit message management
3. **Manual message history** - Traditional approach (still valid, most control)

**Key Insight:** There is NO true "zero-manual-effort" solution yet. All approaches require some level of state management on your end. However, the **Conversations API** is the closest to the automated solution you're looking for.

**Current State (2025):**
- **Responses API** is the new primary API (recommended for all new projects)
- **Chat Completions API** continues to be supported but is legacy
- **Assistants API** is deprecated (sunset date: August 26, 2026)

---

## 1. Current State of OpenAI APIs (2025)

### API Hierarchy

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           RESPONSES API (NEW)                    ‚îÇ
‚îÇ  - Unified interface for agent-like apps        ‚îÇ
‚îÇ  - Built-in tools (web search, file search)     ‚îÇ
‚îÇ  - Native conversation support                   ‚îÇ
‚îÇ  - Recommended for all new projects              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚ñ≤
                      ‚îÇ Replaces
                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         CHAT COMPLETIONS API (LEGACY)           ‚îÇ
‚îÇ  - Still fully supported                         ‚îÇ
‚îÇ  - Will receive new models                       ‚îÇ
‚îÇ  - Manual message management required            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         ASSISTANTS API (DEPRECATED)              ‚îÇ
‚îÇ  - Sunset: August 26, 2026                       ‚îÇ
‚îÇ  - Migration guide available                     ‚îÇ
‚îÇ  - Being replaced by Responses API               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### What Happened to Assistants API?

The Assistants API introduced concepts like:
- **Assistants** (configuration bundles) ‚Üí Now **Prompts** (dashboard-only)
- **Threads** (message storage) ‚Üí Now **Conversations** (item storage)
- **Runs** (async execution) ‚Üí Now **Responses** (sync by default)

**Why the change?**
- Developer feedback showed Assistants API was too complex
- Better performance and flexibility needed
- Responses API is a simpler, more powerful evolution

---

## 2. The Three Native Approaches

### Approach 1: Conversations API (BETA - Most Native)

**What it is:** Server-side conversation objects that persist state automatically.

**How it works:**
1. Create a conversation object once
2. Pass the conversation ID to subsequent responses
3. OpenAI stores all items (messages, tool calls, outputs) server-side
4. No manual message array management needed

**Code Example:**

```typescript
import OpenAI from 'openai';
const openai = new OpenAI();

// Create a conversation once
const conversation = await openai.conversations.create();

// First turn
const response1 = await openai.responses.create({
  model: "gpt-4.1",
  input: [{ role: "user", content: "What are the 5 Ds of dodgeball?" }],
  conversation: conversation.id // üëà Pass conversation ID
});

console.log(response1.output_text);

// Second turn - no need to pass message history!
const response2 = await openai.responses.create({
  model: "gpt-4.1",
  input: [{ role: "user", content: "Tell me more about the first D" }],
  conversation: conversation.id // üëà Same conversation ID
});

console.log(response2.output_text);
```

**Key Features:**
- ‚úÖ Server-side storage (no 30-day TTL like responses)
- ‚úÖ Stores items (messages, tool calls, outputs, etc.)
- ‚úÖ Supports metadata for organizing conversations
- ‚úÖ Can be used across sessions and devices
- ‚ùå You still need to store the conversation ID (e.g., in your database)

**Persistence:**
- Conversation objects: **No expiration** (persist indefinitely)
- Response objects attached to conversations: **No expiration**
- Response objects without conversations: **30 days** (when `store: true`)

**API Reference:**
```typescript
// Create conversation
const conversation = await openai.conversations.create({
  items: [{ role: "user", content: "Initial message" }], // optional
  metadata: { user_id: "user_123" } // optional
});

// Use conversation
const response = await openai.responses.create({
  model: "gpt-4.1",
  conversation: conversation.id,
  input: [{ role: "user", content: "Follow-up question" }]
});
```

**Limitations:**
- ‚ö†Ô∏è Currently in BETA
- ‚ö†Ô∏è You must manage conversation IDs (store in your database)
- ‚ö†Ô∏è No automatic conversation creation on first message
- ‚ö†Ô∏è You need to implement your own conversation retrieval logic

---

### Approach 2: `previous_response_id` Chaining (Simple)

**What it is:** Link responses together by passing the previous response's ID.

**How it works:**
1. Make a response, get a response ID
2. Use that ID in the next request via `previous_response_id`
3. OpenAI retrieves the previous context automatically

**Code Example:**

```typescript
import OpenAI from 'openai';
const openai = new OpenAI();

// First turn
const response1 = await openai.responses.create({
  model: "gpt-4o-mini",
  input: "tell me a joke",
  store: true // üëà Required for previous_response_id
});

console.log(response1.output_text);
console.log(`Response ID: ${response1.id}`);

// Second turn
const response2 = await openai.responses.create({
  model: "gpt-4o-mini",
  previous_response_id: response1.id, // üëà Pass previous response ID
  input: [{ role: "user", content: "explain why this is funny" }],
  store: true
});

console.log(response2.output_text);
```

**Key Features:**
- ‚úÖ Simple chaining without managing message arrays
- ‚úÖ Works with `store: true` responses
- ‚úÖ Good for linear conversations
- ‚ùå Limited to 30-day retention (responses expire)
- ‚ùå You still need to store response IDs
- ‚ùå All previous tokens are billed as input tokens

**Persistence:**
- Response objects: **30 days by default** (when `store: true`)
- Can view in dashboard logs
- Setting `store: false` disables persistence

**Limitations:**
- ‚ö†Ô∏è Not suitable for long-lived conversations (30-day limit)
- ‚ö†Ô∏è Linear chaining only (no branching)
- ‚ö†Ô∏è You must track response IDs
- ‚ö†Ô∏è Previous responses must be stored (`store: true`)

---

### Approach 3: Manual Message History (Traditional)

**What it is:** You manage the message array yourself, passing full history with each request.

**How it works:**
1. Maintain a messages array in your application
2. Append user messages
3. Append assistant responses
4. Pass the entire array with each request

**Code Example:**

```typescript
import OpenAI from 'openai';
const openai = new OpenAI();

let history = [
  { role: "user", content: "tell me a joke" }
];

// First turn
const response1 = await openai.responses.create({
  model: "gpt-4o-mini",
  input: history,
  store: false // Optional: can disable storage
});

// Add response to history
history = [
  ...history,
  ...response1.output.map(el => ({
    role: el.role,
    content: el.content
  }))
];

// Add new user message
history.push({ role: "user", content: "tell me another" });

// Second turn
const response2 = await openai.responses.create({
  model: "gpt-4o-mini",
  input: history
});
```

**Key Features:**
- ‚úÖ Complete control over conversation history
- ‚úÖ Works with or without storage
- ‚úÖ No 30-day limitations (you manage persistence)
- ‚úÖ Can prune/edit history as needed
- ‚ùå You manage all state
- ‚ùå You handle database storage
- ‚ùå More code to write

**Best for:**
- Long-lived conversations
- Complex conversation flows
- When you need full control
- When you're already storing messages in your database

---

## 3. Responses API Deep Dive

### What is the Responses API?

The **Responses API** is OpenAI's new unified interface for building agent-like applications. It replaces both Chat Completions and Assistants API for most use cases.

**Key Benefits:**
- **Better performance:** 3% improvement on SWE-bench vs Chat Completions
- **Lower costs:** 40-80% better cache utilization
- **Agentic by default:** Built-in tool loop (web search, file search, code interpreter)
- **Stateful context:** Native conversation support with `store: true`
- **Flexible inputs:** Pass string or message array
- **Future-proof:** Will receive all new model capabilities first

### API Comparison

```typescript
// Chat Completions (Legacy)
const completion = await openai.chat.completions.create({
  model: "gpt-5",
  messages: [
    { role: "system", content: "You are a helpful assistant." },
    { role: "user", content: "Hello!" }
  ]
});
console.log(completion.choices[0].message.content);

// Responses API (New)
const response = await openai.responses.create({
  model: "gpt-5",
  instructions: "You are a helpful assistant.",
  input: "Hello!"
});
console.log(response.output_text); // üëà Convenient helper
```

### Built-in Tools

The Responses API includes native tools that work out of the box:

```typescript
const response = await openai.responses.create({
  model: "gpt-5",
  input: "Who is the current president of France?",
  tools: [
    { type: "web_search" }, // üëà Built-in web search
    { type: "file_search", vector_store_ids: ["vs_123"] },
    { type: "code_interpreter" }
  ]
});
```

**Available Tools:**
- **web_search:** Real-time web search with citations
- **file_search:** Vector store search for RAG
- **code_interpreter:** Execute Python code
- **computer_use:** Browser automation (preview)
- **image_generation:** Generate images with DALL-E

### Storage and Retention

```typescript
// Enable storage (default for Responses API)
const response = await openai.responses.create({
  model: "gpt-5",
  input: "Hello",
  store: true // üëà Stores for 30 days
});

// Disable storage (for sensitive data)
const response = await openai.responses.create({
  model: "gpt-5",
  input: "Sensitive data here",
  store: false // üëà Not stored server-side
});
```

**Storage Rules:**
- Response objects: **30 days by default** (can view in dashboard logs)
- Response with conversation: **No expiration** (conversation persists)
- `store: false`: No storage (for ZDR compliance)

---

## 4. Vercel AI SDK Integration

### Does Vercel AI SDK Support OpenAI's Native Features?

**Current Status (AI SDK 5.0):**

The Vercel AI SDK **does support** OpenAI's Responses API through the `@ai-sdk/openai` provider:

```typescript
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

// Uses Responses API by default (since AI SDK 5)
const result = await generateText({
  model: openai('gpt-5'),
  prompt: 'Tell me a joke'
});
```

**Supported OpenAI Features:**

‚úÖ **Responses API** (default in AI SDK 5)
‚úÖ **`previous_response_id`** (via provider options)
‚úÖ **Conversations API** (via provider options)
‚úÖ **Built-in tools** (web_search, file_search, etc.)
‚úÖ **Structured outputs**
‚úÖ **Streaming**

### Using `previous_response_id` with Vercel AI SDK

```typescript
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';

const result1 = await generateText({
  model: openai('gpt-5'),
  prompt: 'Tell me a joke',
  providerOptions: {
    openai: {
      store: true
    }
  }
});

// Get the response ID from metadata
const responseId = result1.providerMetadata?.openai?.responseId;

// Continue conversation
const result2 = await generateText({
  model: openai('gpt-5'),
  prompt: 'Explain why it is funny',
  providerOptions: {
    openai: {
      previousResponseId: responseId,
      store: true
    }
  }
});
```

### Using Conversations API with Vercel AI SDK

```typescript
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';
import OpenAI from 'openai';

// Create conversation with OpenAI SDK
const openaiClient = new OpenAI();
const conversation = await openaiClient.conversations.create();

// Use with Vercel AI SDK
const result = await generateText({
  model: openai('gpt-5'),
  prompt: 'Hello!',
  providerOptions: {
    openai: {
      conversation: conversation.id
    }
  }
});
```

### Using `useChat` Hook with Native Features

The `useChat` hook in AI SDK 5.0 uses a **transport-based architecture** but does NOT automatically handle OpenAI's native conversation features. You need to:

1. **Store conversation/response IDs** in your backend
2. **Pass them via request body** to your API route
3. **Use them in your `streamText` call**

**Example: Next.js API Route with Conversations API**

```typescript
// app/api/chat/route.ts
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import OpenAI from 'openai';

const openaiClient = new OpenAI();

export async function POST(req: Request) {
  const { messages, conversationId } = await req.json();

  // Get or create conversation
  let conversation = conversationId;
  if (!conversation) {
    const conv = await openaiClient.conversations.create();
    conversation = conv.id;
    // TODO: Store conversation.id in your database
  }

  const result = streamText({
    model: openai('gpt-5'),
    messages,
    providerOptions: {
      openai: {
        conversation: conversation
      }
    }
  });

  return result.toDataStreamResponse({
    headers: {
      'X-Conversation-Id': conversation // Send back to client
    }
  });
}
```

**Client Side:**

```typescript
'use client';
import { useChat } from '@ai-sdk/react';
import { useState } from 'react';

export default function Chat() {
  const [conversationId, setConversationId] = useState<string | null>(null);

  const { messages, input, handleInputChange, handleSubmit } = useChat({
    body: {
      conversationId // Send to API route
    },
    onResponse: (response) => {
      // Extract conversation ID from response
      const convId = response.headers.get('X-Conversation-Id');
      if (convId) setConversationId(convId);
    }
  });

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>
          <strong>{m.role}:</strong> {m.content}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} />
        <button type="submit">Send</button>
      </form>
    </div>
  );
}
```

### Key Limitation

‚ö†Ô∏è **The Vercel AI SDK does NOT automatically persist conversations for you.**

You still need to:
- Store conversation/response IDs in your database
- Associate them with user sessions
- Pass them between requests
- Handle cleanup/expiration

The SDK makes the API calls easier, but **conversation management is still your responsibility**.

---

## 5. OpenAI's Official Starter App

OpenAI provides an official Next.js starter app that demonstrates best practices:

**Repository:** https://github.com/openai/openai-responses-starter-app

**Features:**
- ‚úÖ Multi-turn conversation handling
- ‚úÖ Streaming responses & tool calls
- ‚úÖ Function calling
- ‚úÖ Display annotations
- ‚úÖ Web search tool configuration
- ‚úÖ Vector store creation & file upload
- ‚úÖ MCP server configuration
- ‚úÖ Google Calendar & Gmail integration

**How it handles conversations:**
The starter app uses **manual message history management** stored in React state. It does NOT use the Conversations API or `previous_response_id`.

**Key Code Pattern:**

```typescript
// Store messages in state
const [messages, setMessages] = useState<Message[]>([]);

// Send message
const sendMessage = async (content: string) => {
  const newMessages = [...messages, { role: 'user', content }];
  setMessages(newMessages);

  const response = await fetch('/api/responses', {
    method: 'POST',
    body: JSON.stringify({ messages: newMessages })
  });

  // Stream and update messages
  // ...
};
```

This is the **traditional approach** (Approach 3), which gives full control but requires manual state management.

---

## 6. Production-Ready Patterns

### Pattern 1: Database-Backed Conversations (Recommended)

**Best for:** Production applications with user accounts

```typescript
// Database schema (Prisma example)
model Conversation {
  id              String    @id @default(cuid())
  userId          String
  openaiConvId    String?   // OpenAI Conversation API ID
  title           String?
  createdAt       DateTime  @default(now())
  updatedAt       DateTime  @updatedAt
  messages        Message[]
}

model Message {
  id              String       @id @default(cuid())
  conversationId  String
  role            String       // 'user' | 'assistant'
  content         String
  createdAt       DateTime     @default(now())
  conversation    Conversation @relation(fields: [conversationId], references: [id])
}
```

**Implementation:**

```typescript
// app/api/chat/route.ts
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';
import { db } from '@/lib/db';
import OpenAI from 'openai';

const openaiClient = new OpenAI();

export async function POST(req: Request) {
  const { conversationId, message } = await req.json();

  // Get or create conversation
  let conversation = await db.conversation.findUnique({
    where: { id: conversationId },
    include: { messages: true }
  });

  if (!conversation) {
    // Create new conversation
    const openaiConv = await openaiClient.conversations.create();
    conversation = await db.conversation.create({
      data: {
        userId: req.user.id, // From auth
        openaiConvId: openaiConv.id
      }
    });
  }

  // Save user message
  await db.message.create({
    data: {
      conversationId: conversation.id,
      role: 'user',
      content: message
    }
  });

  // Stream response
  const result = streamText({
    model: openai('gpt-5'),
    messages: [
      ...conversation.messages.map(m => ({
        role: m.role,
        content: m.content
      })),
      { role: 'user', content: message }
    ],
    providerOptions: {
      openai: {
        conversation: conversation.openaiConvId
      }
    },
    onFinish: async ({ text }) => {
      // Save assistant message
      await db.message.create({
        data: {
          conversationId: conversation.id,
          role: 'assistant',
          content: text
        }
      });
    }
  });

  return result.toDataStreamResponse();
}
```

**Pros:**
- ‚úÖ Full control over data
- ‚úÖ Can display conversation history
- ‚úÖ Searchable conversations
- ‚úÖ Export/backup capabilities
- ‚úÖ Analytics and monitoring

**Cons:**
- ‚ùå More code to write
- ‚ùå Database storage costs
- ‚ùå Need to handle cleanup

---

### Pattern 2: Session-Based (Simple)

**Best for:** Anonymous chat interfaces, demos, prototypes

```typescript
// Use session storage or cookies
import { cookies } from 'next/headers';

export async function POST(req: Request) {
  const cookieStore = cookies();
  const conversationId = cookieStore.get('conversation_id')?.value;

  let conversation = conversationId;
  if (!conversation) {
    const conv = await openai.conversations.create();
    conversation = conv.id;
    cookieStore.set('conversation_id', conversation);
  }

  // Use conversation ID...
}
```

**Pros:**
- ‚úÖ No database required
- ‚úÖ Quick to implement
- ‚úÖ Good for demos

**Cons:**
- ‚ùå Lost when cookies cleared
- ‚ùå Can't sync across devices
- ‚ùå No history/search

---

### Pattern 3: Hybrid (Best of Both)

**Best for:** Applications that need both persistence and performance

```typescript
// Use OpenAI Conversations API for state management
// Use your database only for metadata and search
model Conversation {
  id              String    @id @default(cuid())
  userId          String
  openaiConvId    String    @unique // OpenAI manages the messages
  title           String?
  metadata        Json?     // For search/filtering
  createdAt       DateTime  @default(now())
  updatedAt       DateTime  @updatedAt
}

// Messages are NOT stored in your database
// They're retrieved from OpenAI when needed
```

**Benefits:**
- Reduce database storage costs
- Leverage OpenAI's infrastructure
- Still have searchable metadata
- Simpler schema

---

## 7. Pros & Cons Comparison

### Conversations API

**Pros:**
- ‚úÖ Server-side persistence (no expiration)
- ‚úÖ Simplified state management
- ‚úÖ Native OpenAI integration
- ‚úÖ Stores all item types (messages, tools, etc.)
- ‚úÖ Good for long-lived conversations

**Cons:**
- ‚ùå Still BETA (may change)
- ‚ùå You must store conversation IDs
- ‚ùå No automatic conversation creation
- ‚ùå Requires additional API calls (create conversation)
- ‚ùå Less control over conversation flow

**Best for:**
- Applications with user accounts
- Long-lived conversations
- Multi-device support

---

### `previous_response_id` Chaining

**Pros:**
- ‚úÖ Simple to implement
- ‚úÖ No explicit message management
- ‚úÖ Native OpenAI feature
- ‚úÖ Works with streaming

**Cons:**
- ‚ùå 30-day retention limit
- ‚ùå You must store response IDs
- ‚ùå Linear chaining only
- ‚ùå All previous tokens billed
- ‚ùå Requires `store: true`

**Best for:**
- Short-term conversations
- Linear conversation flows
- Quick prototypes

---

### Manual Message History

**Pros:**
- ‚úÖ Complete control
- ‚úÖ No retention limits (you manage persistence)
- ‚úÖ Works with all models/APIs
- ‚úÖ Can edit/prune history
- ‚úÖ Portable across providers
- ‚úÖ Most mature approach

**Cons:**
- ‚ùå More code to write
- ‚ùå You handle all state management
- ‚ùå Database storage required
- ‚ùå Need to implement pruning/cleanup

**Best for:**
- Production applications
- Complex conversation flows
- When you need full control
- Long-term persistence

---

## 8. Migration Guides

### From Assistants API to Responses API

OpenAI provides an official migration guide: https://platform.openai.com/docs/assistants/migration

**Key Changes:**

| Before (Assistants) | Now (Responses) | Why? |
|---------------------|-----------------|------|
| Assistants | Prompts | Prompts are versioned and dashboard-managed |
| Threads | Conversations | Conversations store items, not just messages |
| Runs | Responses | Responses are synchronous by default |
| Run steps | Items | Generalized objects for messages, tool calls, etc. |

**Migration Steps:**

1. **Create prompts from assistants:**
   - Identify assistant objects in your application
   - Create equivalent prompts in OpenAI dashboard
   - Store prompt IDs in your application

2. **Move conversations to Conversations API:**
   - For new chats, use `openai.conversations.create()`
   - For existing threads, backfill manually (no auto-migration tool)

3. **Update response logic:**
   - Replace `openai.beta.threads.runs.create()` with `openai.responses.create()`
   - Remove polling logic (responses are synchronous)

**Example Conversion:**

```typescript
// Before (Assistants API)
const thread = await openai.beta.threads.create();
const run = await openai.beta.threads.runs.create({
  thread_id: thread.id,
  assistant_id: "asst_123"
});

// Poll for completion
while (run.status === "queued" || run.status === "in_progress") {
  await new Promise(resolve => setTimeout(resolve, 1000));
  run = await openai.beta.threads.runs.retrieve(thread.id, run.id);
}

// After (Responses API)
const conversation = await openai.conversations.create();
const response = await openai.responses.create({
  model: "gpt-4.1",
  conversation: conversation.id,
  input: [{ role: "user", content: "Hello" }]
});
// Done! No polling needed
```

---

### From Chat Completions to Responses API

OpenAI provides an official migration guide: https://platform.openai.com/docs/guides/migrate-to-responses

**Key Changes:**

- `messages` ‚Üí `input` (can be string or array)
- `system` message ‚Üí `instructions` (top-level)
- `choices[0].message.content` ‚Üí `output_text` (helper)
- `response_format` ‚Üí `text.format` (for structured outputs)

**Example:**

```typescript
// Before (Chat Completions)
const completion = await openai.chat.completions.create({
  model: "gpt-5",
  messages: [
    { role: "system", content: "You are helpful." },
    { role: "user", content: "Hello" }
  ]
});
const text = completion.choices[0].message.content;

// After (Responses API)
const response = await openai.responses.create({
  model: "gpt-5",
  instructions: "You are helpful.",
  input: "Hello"
});
const text = response.output_text;
```

**Note:** Chat Completions will continue to be supported, so migration is optional.

---

## 9. Recommended Setup (2025)

### The Simplest Production-Ready Approach

**Recommendation:** Use **Conversations API + Database Metadata**

**Why:**
1. Leverage OpenAI's infrastructure for message storage
2. Use your database for searchable metadata
3. Balance simplicity and control
4. Future-proof (aligned with OpenAI's direction)

**Implementation:**

```typescript
// 1. Database Schema (minimal)
model Conversation {
  id           String   @id @default(cuid())
  userId       String
  openaiConvId String   @unique
  title        String?
  createdAt    DateTime @default(now())
  updatedAt    DateTime @updatedAt

  @@index([userId, createdAt])
}

// 2. API Route
export async function POST(req: Request) {
  const { conversationId, message } = await req.json();
  const userId = await getUserId(req);

  // Get or create conversation
  let conversation = await db.conversation.findUnique({
    where: { id: conversationId }
  });

  if (!conversation) {
    const openaiConv = await openai.conversations.create({
      metadata: { userId }
    });

    conversation = await db.conversation.create({
      data: {
        userId,
        openaiConvId: openaiConv.id,
        title: message.slice(0, 50) // Auto-title
      }
    });
  }

  // Stream response using Vercel AI SDK
  const result = streamText({
    model: openai('gpt-5'),
    messages: [{ role: 'user', content: message }],
    providerOptions: {
      openai: {
        conversation: conversation.openaiConvId
      }
    }
  });

  return result.toDataStreamResponse();
}

// 3. Client (Next.js with useChat)
export default function Chat({ conversationId }: { conversationId?: string }) {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    body: { conversationId }
  });

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>{m.content}</div>
      ))}
      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} />
        <button type="submit">Send</button>
      </form>
    </div>
  );
}
```

**Benefits:**
- ‚úÖ Minimal database storage
- ‚úÖ Leverage OpenAI's infrastructure
- ‚úÖ Simple to implement
- ‚úÖ Scalable
- ‚úÖ Future-proof

---

## 10. Key Takeaways

1. **There is NO fully automatic solution yet.** All approaches require you to manage conversation/response IDs.

2. **Conversations API is the closest to "native"**, but it's still in BETA and requires ID management.

3. **Responses API is the future.** It's recommended for all new projects.

4. **Assistants API is deprecated.** Migrate before August 26, 2026.

5. **Vercel AI SDK works well with OpenAI**, but doesn't automatically handle conversation persistence.

6. **For production, use Conversations API + Database Metadata** for the best balance.

7. **Manual message history is still valid** if you need full control.

8. **Storage matters:**
   - Responses: 30 days (with `store: true`)
   - Conversations: No expiration
   - Your database: You control

9. **Cost considerations:**
   - All previous tokens are billed when using `previous_response_id`
   - Conversations API has no additional cost beyond standard API usage
   - Database storage is your responsibility

10. **The "zero-effort" dream isn't here yet**, but OpenAI is moving in that direction with the Conversations API.

---

## 11. Additional Resources

### Official Documentation
- [Responses API Guide](https://platform.openai.com/docs/api-reference/responses)
- [Conversation State Guide](https://platform.openai.com/docs/guides/conversation-state)
- [Assistants Migration Guide](https://platform.openai.com/docs/assistants/migration)
- [Responses API Migration Guide](https://platform.openai.com/docs/guides/migrate-to-responses)

### Official Examples
- [OpenAI Responses Starter App](https://github.com/openai/openai-responses-starter-app)
- [Vercel AI SDK Examples](https://github.com/vercel/ai/tree/main/examples)

### Community Resources
- [Vercel AI SDK Discussions](https://github.com/vercel/ai/discussions)
- [OpenAI Community Forum](https://community.openai.com/)

### Related Guides
- [OpenAI Data Controls](https://platform.openai.com/docs/guides/your-data)
- [Vercel AI SDK useChat Hook](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat)
- [OpenAI Provider (Vercel AI SDK)](https://ai-sdk.dev/providers/ai-sdk-providers/openai)

---

## Conclusion

Multi-turn conversation management with OpenAI in 2025 is **simpler than before** thanks to the Responses API and Conversations API, but it's **not yet zero-effort**.

**The recommended path:**
1. Use the **Responses API** (not Chat Completions)
2. Use the **Conversations API** for state management
3. Store conversation IDs in your database
4. Use **Vercel AI SDK** for easy integration
5. Leverage **built-in tools** (web search, file search) when needed

This gives you the best balance of simplicity, control, and future-proofing while aligning with OpenAI's direction.

The true "zero-effort" solution‚Äîwhere OpenAI automatically manages everything including user sessions‚Äîdoesn't exist yet. But the Conversations API is a significant step in that direction.
